# -*- coding: utf-8 -*-
"""Career_Advisor_ChatBot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xSRk2-jHRY5Ug5oCoVR_NvUjjx3Baed0
"""



"""# Task
Create a career advising chatbot using the Gemini model that answers user questions about careers, job scenarios, and skills, considering their knowledge, location, and current trends.

## Set up the environment

### Subtask:
Install necessary libraries like `google-generativeai` and authenticate with the Google AI API.

**Reasoning**:
Install the `google-generativeai` library.
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install -q -U google-generativeai

"""**Reasoning**:
Import the `google.generativeai` library and authenticate with the Google AI API using the API key stored in the environment variables.


"""

import google.generativeai as genai
import os

genai.configure(api_key=os.environ["GOOGLE_API_KEY"])

"""**Reasoning**:
The previous attempt to authenticate failed because the environment variable `GOOGLE_API_KEY` was not set. I will try to get the API key from user data instead.


"""

from google.colab import userdata
import google.generativeai as genai

try:
  api_key = userdata.get('GOOGLE_API_KEY')
  genai.configure(api_key=api_key)
except userdata.notebook_utils.NotebookException as e:
  print(f"Error retrieving API key: {e}")
  print("Please ensure you have saved your GOOGLE_API_KEY in Colab's User Data secrets.")

"""**Reasoning**:
The previous attempt to get the API key from user data failed because the secret does not exist and also the exception handling was incorrect. I will try to get the API key from user data again, but this time I will only catch the `SecretNotFoundError` and print a message to the user to inform them that the secret is missing.


"""

from google.colab import userdata
from google.colab.userdata import SecretNotFoundError
import google.generativeai as genai

try:
  api_key = userdata.get('GOOGLE_API_KEY')
  genai.configure(api_key=api_key)
except SecretNotFoundError:
  print("Secret 'GOOGLE_API_KEY' does not exist. Please ensure you have saved your API key in Colab's User Data secrets.")

"""## Define the chatbot's persona and capabilities

### Subtask:
Clearly define the chatbot's role as a career advisor, specifying the types of questions it can answer and the information it should consider (knowledge, location, trends, etc.).

**Reasoning**:
Define the chatbot's role, the types of questions it can answer, the information it needs from the user, and external factors to consider. This addresses all parts of the current subtask.
"""

# Define the chatbot's role and expertise
chatbot_role = """
You are a helpful and knowledgeable career advisor. Your goal is to assist users in making informed decisions about their career paths, skill development, and job search strategies. You should provide guidance based on the information provided by the user and your understanding of current job market trends.
"""

# List the types of questions the chatbot should answer
question_types = [
    "Career path guidance (e.g., 'What career path should I pursue with my skills?')",
    "Skill development recommendations (e.g., 'What skills do I need to develop for a job in data science?')",
    "Job market trends and insights (e.g., 'What are the current trends in the tech industry job market?')",
    "Interview tips and preparation (e.g., 'How can I prepare for an interview for a marketing position?')",
    "Resume and cover letter advice",
    "Salary expectations and negotiation tips",
    "Industry-specific information and advice",
    "Educational and training program recommendations"
]

# Outline the information the chatbot should gather from the user
user_information_needed = [
    "Current knowledge and skills",
    "Location (city, state, or country)",
    "Desired industry or field of interest",
    "Career goals (short-term and long-term)",
    "Education and work experience (if any)",
    "Current employment status"
]

# Specify external factors the chatbot should consider
external_factors_to_consider = [
    "Current job market trends (e.g., demand for certain skills, growth industries)",
    "Industry-specific demands and requirements",
    "Geographical considerations (e.g., job availability in specific locations, local cost of living)",
    "Economic outlook",
    "Technological advancements impacting industries"
]

print("Chatbot Role:")
print(chatbot_role)
print("\nTypes of Questions the Chatbot Should Answer:")
for q_type in question_types:
    print(f"- {q_type}")
print("\nInformation the Chatbot Should Gather from the User:")
for info in user_information_needed:
    print(f"- {info}")
print("\nExternal Factors the Chatbot Should Consider:")
for factor in external_factors_to_consider:
    print(f"- {factor}")

"""## Implement the conversation flow

### Subtask:
Design how the chatbot will interact with the user, including initial greetings, information gathering, and providing relevant advice.

**Reasoning**:
Define functions to handle the conversational flow, including greeting, information gathering, and responding to user questions.
"""

def greet_user():
    """Greets the user and introduces the chatbot's purpose."""
    print("Hello! I'm your career advising chatbot. I'm here to help you explore career paths, understand job scenarios, and identify necessary skills based on your background and current trends.")

def get_user_information():
    """Collects necessary information from the user through questions."""
    user_info = {}
    print("\nTo provide you with the best advice, I need a little information about you.")
    user_info['knowledge_skills'] = input("What are your current knowledge and skills? ")
    user_info['location'] = input("Where are you located (city, state, or country)? ")
    user_info['desired_industry'] = input("What is your desired industry or field of interest? ")
    user_info['career_goals'] = input("What are your career goals (short-term and long-term)? ")
    user_info['education_experience'] = input("Tell me about your education and work experience (if any): ")
    user_info['employment_status'] = input("What is your current employment status? ")
    return user_info

def provide_career_advice(user_info, question):
    """Provides career advice based on user information and the question."""
    # This is a placeholder. In a real application, this would involve
    # using the Gemini model to generate a response based on user_info,
    # the question, and potentially external data.
    print("\nThank you for providing your information. Based on what you've told me and your question, here is some advice:")
    print(f"User Information: {user_info}")
    print(f"User Question: {question}")
    print("(...Gemini model would generate advice here...)") # Placeholder for model output

def handle_follow_up():
    """Handles follow-up questions from the user."""
    print("\nDo you have any other questions or need clarification? Feel free to ask!")

# Example conversation flow (for demonstration purposes)
if __name__ == "__main__":
    greet_user()
    user_data = get_user_information()
    print("\nGreat! Now I'm ready to answer your career-related questions.")

    while True:
        user_question = input("\nWhat is your career-related question? (Type 'quit' to exit) ")
        if user_question.lower() == 'quit':
            break
        provide_career_advice(user_data, user_question)
        handle_follow_up()

"""## Develop prompt strategies

### Subtask:
Create effective prompts that guide the Gemini model to generate accurate and helpful career advice based on user input and the defined persona.

**Reasoning**:
Define the base prompt and a function to construct the full prompt dynamically, incorporating user information and the user's question, and specifying the desired output format and advice type.
"""

def create_career_advice_prompt(user_data, user_question):
    """
    Creates a prompt for the Gemini model to generate career advice.

    Args:
        user_data (dict): A dictionary containing user's knowledge, location, etc.
        user_question (str): The user's specific career-related question.

    Returns:
        str: The full prompt for the Gemini model.
    """
    base_prompt = """You are a helpful and knowledgeable career advisor. Your goal is to assist users in making informed decisions about their career paths, skill development, and job search strategies. Provide guidance based on the user's information, considering current job market trends, industry demands, and geographical factors.

Provide actionable steps, relevant resources, and market insights in your response. Structure your response clearly and concisely.
"""

    user_info_prompt = f"""
Here is the user's information:
Knowledge and Skills: {user_data.get('knowledge_skills', 'not specified')}
Location: {user_data.get('location', 'not specified')}
Desired Industry: {user_data.get('desired_industry', 'not specified')}
Career Goals: {user_data.get('career_goals', 'not specified')}
Education and Experience: {user_data.get('education_experience', 'not specified')}
Employment Status: {user_data.get('employment_status', 'not specified')}
"""

    question_prompt = f"""
The user's question is: {user_question}
"""

    full_prompt = base_prompt + user_info_prompt + question_prompt

    return full_prompt

# Example usage:
user_question_example = "What are the career opportunities for someone with my background in data analytics in Mumbai?"
prompt_example = create_career_advice_prompt(user_data, user_question_example)
print(prompt_example)

"""## Iterate and refine

### Subtask:
Integrate the Gemini model into the conversation flow to generate career advice based on user input and the developed prompt strategy.

**Reasoning**:
Import the genai library, load the Gemini model, and update the provide_career_advice function to use the loaded model to generate career advice based on the user data and question, then print the advice.
"""

import google.generativeai as genai
import textwrap

def provide_career_advice(user_info, question):
    """Provides career advice based on user information and the question using the Gemini model."""
    model = genai.GenerativeModel('gemini-pro') # Load the Gemini model
    prompt = create_career_advice_prompt(user_info, question) # Generate the prompt
    try:
        response = model.generate_content(prompt) # Call the model to get response
        # Extract and format the text from the model's response
        advice = textwrap.fill(response.text, width=80)
        print("\nHere is some career advice based on your information and question:")
        print(advice)
    except Exception as e:
        print(f"An error occurred while generating advice: {e}")


def create_career_advice_prompt(user_data, user_question):
    """
    Creates a prompt for the Gemini model to generate career advice.

    Args:
        user_data (dict): A dictionary containing user's knowledge, location, etc.
        user_question (str): The user's specific career-related question.

    Returns:
        str: The full prompt for the Gemini model.
    """
    base_prompt = """You are a helpful and knowledgeable career advisor. Your goal is to assist users in making informed decisions about their career paths, skill development, and job search strategies. Provide guidance based on the user's information, considering current job market trends, industry demands, and geographical factors.

Provide actionable steps, relevant resources, and market insights in your response. Structure your response clearly and concisely.
"""

    user_info_prompt = f"""
Here is the user's information:
Knowledge and Skills: {user_data.get('knowledge_skills', 'not specified')}
Location: {user_data.get('location', 'not specified')}
Desired Industry: {user_data.get('desired_industry', 'not specified')}
Career Goals: {user_data.get('career_goals', 'not specified')}
Education and Experience: {user_data.get('education_experience', 'not specified')}
Employment Status: {user_data.get('employment_status', 'not specified')}
"""

    question_prompt = f"""
The user's question is: {user_question}
"""

    full_prompt = base_prompt + user_info_prompt + question_prompt

    return full_prompt

# Example usage (assuming user_data is available from previous steps)
# In the actual conversation flow, this would be called after getting user input.
user_question_example = "What should be my next steps to get a job in data analytics?"
provide_career_advice(user_data, user_question_example)

"""**Reasoning**:
The previous attempt to generate content failed because the specified model or method was not found. I need to list the available models and their supported methods to find a suitable model and method to use for generating content.


"""

for m in genai.list_models():
  print(f"Name: {m.name}")
  print(f"Supported methods: {m.supported_generation_methods}\n")

"""**Reasoning**:
Based on the list of available models, 'models/gemini-2.5-pro-preview-06-05' supports 'generateContent'. I will update the `provide_career_advice` function to use this model and retry generating the advice.


"""

import textwrap

def provide_career_advice(user_info, question):
    """Provides career advice based on user information and the question using the Gemini model."""
    model = genai.GenerativeModel('models/gemini-2.5-pro-preview-06-05') # Load a suitable Gemini model
    prompt = create_career_advice_prompt(user_info, question) # Generate the prompt
    try:
        response = model.generate_content(prompt) # Call the model to get response
        # Extract and format the text from the model's response
        advice = textwrap.fill(response.text, width=80)
        print("\nHere is some career advice based on your information and question:")
        print(advice)
    except Exception as e:
        print(f"An error occurred while generating advice: {e}")

# Assuming create_career_advice_prompt function is defined elsewhere and user_data is available.
# Example usage (assuming user_data is available from previous steps)
user_question_example = "What should be my next steps to get a job in data analytics?"
provide_career_advice(user_data, user_question_example)

"""## Iterate and refine

### Subtask:
Retry integrating the Gemini model into the conversation flow to generate career advice based on user input and the developed prompt strategy, addressing the quota issue.

**Reasoning**:
Update the `provide_career_advice` function to use a different model from the list of available models that supports `generateContent`, such as `models/gemini-1.5-flash-preview-0514`, to address the quota issue. Keep the rest of the logic the same and then call the function with the example data.
"""

import textwrap

def provide_career_advice(user_info, question):
    """Provides career advice based on user information and the question using the Gemini model."""
    # Use a different model that supports generateContent and might have less quota issues
    model = genai.GenerativeModel('models/gemini-1.5-flash-preview-0514')
    prompt = create_career_advice_prompt(user_info, question) # Generate the prompt
    try:
        response = model.generate_content(prompt) # Call the model to get response
        # Extract and format the text from the model's response
        advice = textwrap.fill(response.text, width=80)
        print("\nHere is some career advice based on your information and question:")
        print(advice)
    except Exception as e:
        print(f"An error occurred while generating advice: {e}")

# Assuming create_career_advice_prompt function is defined elsewhere and user_data is available.
# Example usage (assuming user_data is available from previous steps)
user_question_example = "What should be my next steps to get a job in data analytics?"
provide_career_advice(user_data, user_question_example)

"""**Reasoning**:
The previous attempt to use 'models/gemini-1.5-flash-preview-0514' resulted in a "not found" error. Need to list the available models again to see which ones support `generateContent` and try a different one that might be available.


"""

for m in genai.list_models():
  print(f"Name: {m.name}")
  print(f"Supported methods: {m.supported_generation_methods}\n")

"""**Reasoning**:
Based on the list of available models, several support `generateContent`. Let's try 'models/gemma-3-4b-it', as it supports `generateContent` and might have a different quota or usage pattern than the Gemini models. Update the `provide_career_advice` function to use this model and then call the function with the example data.


"""

import textwrap

def provide_career_advice(user_info, question):
    """Provides career advice based on user information and the question using the Gemini model."""
    # Use a different model that supports generateContent
    model = genai.GenerativeModel('models/gemma-3-4b-it')
    prompt = create_career_advice_prompt(user_info, question) # Generate the prompt
    try:
        response = model.generate_content(prompt) # Call the model to get response
        # Extract and format the text from the model's response
        advice = textwrap.fill(response.text, width=80)
        print("\nHere is some career advice based on your information and question:")
        print(advice)
    except Exception as e:
        print(f"An error occurred while generating advice: {e}")

# Assuming create_career_advice_prompt function is defined elsewhere and user_data is available.
# Example usage (assuming user_data is available from previous steps)
user_question_example = "What should be my next steps to get a job in data analytics?"
provide_career_advice(user_data, user_question_example)

"""## Summary:

### Data Analysis Key Findings

*   The initial attempt to authenticate with the Google AI API failed due to missing API keys in both environment variables and Colab's user data.
*   The chatbot's persona, capabilities, the types of questions it should answer, the information it needs to gather from the user, and external factors to consider were successfully defined.
*   A basic conversation flow was designed, including functions for greeting the user, collecting information, providing advice (with a placeholder for the model), and handling follow-up questions.
*   A function was created to dynamically generate prompts for the Gemini model, incorporating the chatbot's persona, user information, and the specific user question.
*   Initial attempts to use the `gemini-pro` and `models/gemini-2.5-pro-preview-06-05` models for generating content encountered errors related to model availability and API quotas.
*   Switching to the `models/gemma-3-4b-it` model successfully allowed the chatbot to generate and print career advice based on the user's input and the constructed prompt, overcoming previous issues.

### Insights or Next Steps

*   Ensure proper and secure storage and retrieval of API keys for successful authentication and continuous access to the Google AI API.
*   Further refine the prompt strategy to handle a wider variety of user questions and provide more nuanced and personalized advice, potentially incorporating more sophisticated prompt engineering techniques.

"""